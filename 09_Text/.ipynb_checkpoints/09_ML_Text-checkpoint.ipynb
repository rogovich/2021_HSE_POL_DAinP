{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Автор: Татьяна Рогович*\n",
    "\n",
    "# Анализ данных в Python\n",
    "\n",
    "## Bag of Words (мешок слов), стемминг и классификация текстов\n",
    "\n",
    "*Автор: Татьяна Рогович, НИУ ВШЭ*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что такое NLP?\n",
    "Обработка естественного языка - (Natural Language Processing, NLP ), представляет собой набор методов для решения задач по обработке текстов. В этом уроке, мы попробуем загрузить и почистить текст новостей, а затем, использую простую модель признаков - мешка слов (Bag of Words), научимся довольно точно предсказывать фэйковая это новость или настоящая.\n",
    "\n",
    "Но для начала посмотрим как работает bag of words на игрушечном примере."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем признаки (features) из Bag of Words\n",
    "\n",
    "Как превратить текст в некоторое численное представление для машинного обучения? Один из таких подходов называется мешком слов (Bag of Words). Модель Bag of Words изучает все слова, которые есть в нашем корпусе, а затем моделирует каждый документ, подсчитывая количество раз, когда появляется каждое слово. Например, рассмотрим следующие два предложения:\n",
    "\n",
    "Предложение 1: \"The cat sat on the hat\"\n",
    "\n",
    "Предложение 2: \"The dog ate the cat and the hat\"\n",
    "\n",
    "Для этих двух сообщений,получим следующий словарь:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "Чтобы получить наш мешок слов, мы подсчитаем сколько раз каждое слово появилось в предложении. В предложение 1, \"the\" появилось два раза, а \"cat\", \"sat\", \"on\", и \"hat\" по одному разу каждый. Поэтому вектор признаков для этого предложения выглядит так:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "{ 2, 1, 1, 1, 1, 0, 0, 0 }\n",
    "\n",
    "Аналогично, признаки предложения 2 будут такими: \n",
    "\n",
    "{ 3, 1, 0, 0, 1, 1, 1, 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как перевести текст в признаки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если наша задача типовая, то скорее всего в sklearn уже есть библиотека, с помощью которой можно ее решить. Мы будем пользоваться классом CountVectorizer(), который при обучении создает признаки из всех слов обучающей выборки, при трансформации - подсчитывает встречание этих слов в тренировочной и тестовой выборке. А потом получившиеся наборы данных мы уже передаем алгоритмам для предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим тренировочный список текстов\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'call 222-22-22', 'please call me.. please']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем и иниициализируем класс CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучаемся на данных\n",
    "vect.fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22', '222', 'cab', 'call', 'me', 'please', 'tonight', 'you']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Смотрим сгенерированные признаки\n",
    "vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в наш векторайзер уже зашита базовая обработка текста. Он приводит текст к нижнему регистру, и с помощью регулярок забирает только слова, состоящие из цифр и букв (знаки препинания удаляются), удаляет стоп-слова.\n",
    "\n",
    "Метод fit здесь разбивает текст на токены и сохраняет их в \"модель\". Метод transform мы используем, чтобы создать разреженную матрицу, в которой мы будем хранить информацию, сколько каждое слово встречалось в каждом \"тексте\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# преобразовываем исходный текст в матрицу признаков\n",
    "simple_train_matrix = vect.transform(simple_train)\n",
    "# каждый ряд - одно наблюдение (наш документ), каждая колонка - один признак (слово). \n",
    "# На пересечении - количество слов в документе.\n",
    "simple_train_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [2, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 2, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_train_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sparse matrix*\n",
    "\n",
    "Разреженная матрица. Хранит только координаты ненулевых значений. Сильно экономит место. \n",
    "\n",
    "*dense matrix*\n",
    "\n",
    "Плотная матрица. Хранит все данные.\n",
    "Если у вас матрица 1000 на 1000 из нулей - она все это хранит (1 мб ничего)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>22</th>\n",
       "      <th>222</th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>call you tonight</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Call me a cab</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call 222-22-22</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>please call me.. please</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         22  222  cab  call  me  please  tonight  you\n",
       "call you tonight          0    0    0     1   0       0        1    1\n",
       "Call me a cab             0    0    1     1   1       0        0    0\n",
       "call 222-22-22            2    1    0     1   0       0        0    0\n",
       "please call me.. please   0    0    0     1   1       2        0    0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(simple_train_matrix.toarray(), columns=vect.get_feature_names(), index=simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 0)\t2\n",
      "  (2, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 5)\t2\n",
      "dense matrix\n",
      "[[0 0 0 1 0 0 1 1]\n",
      " [0 0 1 1 1 0 0 0]\n",
      " [2 1 0 1 0 0 0 0]\n",
      " [0 0 0 1 1 2 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print('sparse matrix')\n",
    "print(simple_train_matrix)\n",
    "\n",
    "print('dense matrix')\n",
    "print(simple_train_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создадим текст для теста и преобразуем обученным CountVectorizer\n",
    "simple_test = ['Please don\\'t call me, I will be busy']\n",
    "simple_test_matrix = vect.transform(simple_test)\n",
    "simple_test_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>22</th>\n",
       "      <th>222</th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Please don't call me, I will be busy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      22  222  cab  call  me  please  tonight  \\\n",
       "Please don't call me, I will be busy   0    0    0     1   1       1        0   \n",
       "\n",
       "                                      you  \n",
       "Please don't call me, I will be busy    0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# куда делся don't?\n",
    "pd.DataFrame(simple_test_matrix.toarray(), columns=vect.get_feature_names(), index=simple_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform для тестовой выборки будет игнорировать токены, которые он не видел раньше (поэтому важен большой размер обучающей выборки и ее репрезентативность корпуса текстов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг (Stemming) и Лемматизация (Lemmatization)\n",
    "\n",
    "Процесс стемминга заключается в преобразовании различных вариантов одного и того же слова в одну ядерную форму. Например у нас есть слова \"running\", \"runs\" и \"run\", которые семантически говорят об одном и том же, поэтому мы можем вместо них использовать одно слово \"run\". Стоит отметить, что в этом случае мы можем потерять некоторые грамматические признаки, например форму времени.\n",
    "\n",
    "Воспользуемся алгоритмом Портера из модуля NLTK. Посмотрим на работу алгоритма на примере с вариантами слов run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rogov\\anaconda3\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\rogov\\anaconda3\\lib\\site-packages (from nltk) (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\rogov\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed form of running is: run\n",
      "The stemmed form of runs is: run\n",
      "The stemmed form of run is: run\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "print(\"The stemmed form of running is: {}\".format(stemmer.stem(\"running\")))\n",
    "print(\"The stemmed form of runs is: {}\".format(stemmer.stem(\"runs\")))\n",
    "print(\"The stemmed form of run is: {}\".format(stemmer.stem(\"run\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С русским языком не работает, к сожалению. Но сам Портер адаптировал алгоритм позже для ряда языков (смотри ниже Snowball stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed form of бегать is: бегать\n"
     ]
    }
   ],
   "source": [
    "print(\"The stemmed form of бегать is: {}\".format(stemmer.stem(\"бегать\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы успешно обрезали наши слова до базовой формы. Алгоритм не использует баз основ слов, а лишь, применяя последовательно ряд правил, отсекает окончания и суффиксы, основываясь на особенностях языка, в связи с чем работает быстро, но не всегда безошибочно.\n",
    "\n",
    "Однако у стемминга есть тенденция к \"грубому\" обрезанию концов слов. Например, для leaves мы получим следующее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed form of leaves is: leav\n"
     ]
    }
   ],
   "source": [
    "print(\"The stemmed form of leaves is: {}\".format(stemmer.stem(\"leaves\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С одной стороны это нормально, но базовой формой слова будет leaf. Поэтому на помощь нам приходит лемматизация.\n",
    "\n",
    "Лемматизация пытается достигнуть того же эффекта, но в отличие от стеммера, лемматизация использует реальный словарь слов и поэтому не будет обрубать окончания слов, а будет возвращать лемму.\n",
    "\n",
    "Снова воспользуемся модулем NLTK и проверим, что он нам выдаст на leaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized form of leaves is: leaf\n"
     ]
    }
   ],
   "source": [
    "# Импортируем\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Инициализируем \n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "print(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили верный ответ. Лемматизация обеспечивает нам более тонкую настройку по сравнению со стеммингом. Но чтобы интегрировать лемматизацию в count vectorizer, очистку текста придется делать сначала вручную (см. ниже)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с русскоязычными текстами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С поддержкой русского языка у nltk не супер, к сожалению. Можно воспользоваться алгоритмом для стемминга, который является адаптацией стеммера Портера (но с русским языком ему сильно тяжелее, чем с английским)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бежа\n",
      "бегущ\n",
      "бега\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "rus_stemmer = RussianStemmer()\n",
    "\n",
    "print(rus_stemmer.stem('бежать'))\n",
    "print(rus_stemmer.stem('бегущий'))\n",
    "print(rus_stemmer.stem('бегающий'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если библиотека pymorphy, которая делает лемматизацию, основываясь на словаре проекта OpenCorpora. https://pymorphy2.readthedocs.io/\n",
    "\n",
    "Далее объяснения и примеры из документации.\n",
    "\n",
    "В pymorphy2 для морфологического анализа слов (русских) есть класс MorphAnalyzer.\n",
    "Экземпляры класса MorphAnalyzer занимают порядка 10-15Мб оперативной памяти (т.к. загружают в память словари, данные для предсказателя и т.д.); старайтесь ораганизовать свой код так, чтоб создавать экземпляр MorphAnalyzer заранее и работать с этим единственным экземпляром в дальнейшем.\n",
    "\n",
    "Метод MorphAnalyzer.parse() принимает слово (обязательно в нижнем регистре) и возвращает все возможные разборы слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse('стали')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в этом примере слово “стали” может быть разобрано и как глагол (“они стали лучше справляться”), и как существительное (“кислородно-конверторный способ получения стали”). На основе одной лишь информации о том, как слово пишется, понять, какой разбор правильный, нельзя, поэтому анализатор может возвращать несколько вариантов разбора.\n",
    "\n",
    "**Выбор правильного разбора**  \n",
    "pymorphy2 возвращает все допустимые варианты разбора, но на практике обычно нужен только один вариант, правильный.\n",
    "\n",
    "У каждого разбора есть параметр score. Score - это оценка P(tag|word), оценка вероятности того, что данный разбор правильный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pymorphy2 умеет разбирать не только словарные слова; для несловарных слов автоматически задействуется предсказатель. Например, попробуем разобрать слово “бутявковедами” - pymorphy2 поймет, что это форма творительного падежа множественного числа существительного “бутявковед”, и что “бутявковед” - одушевленный и мужского рода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='бутявковедами', tag=OpencorporaTag('NOUN,anim,masc plur,ablt'), normal_form='бутявковед', score=1.0, methods_stack=((<FakeDictionary>, 'бутявковедами', 52, 10), (<KnownSuffixAnalyzer>, 'едами')))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('бутявковедами')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У каждого разбора есть нормальная форма, которую можно получить, обратившись к атрибуту normal_form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'стать'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = morph.parse('стали')[0]\n",
    "p.normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pymorphy2 умеет склонять (ставить в какую-то другую форму) слова. Чтобы просклонять слово, его нужно сначала разобрать - понять, в какой форме оно стоит в настоящий момент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бутявка'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butyavka = morph.parse('бутявки')[0]\n",
    "butyavka.normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бутявки'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butyavka.inflect({'gent'})[0] # родительный падеж"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью атрибута lexeme можно получить лексему слова:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='бутявка', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явка', 8, 0), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявки', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явки', 8, 1), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявке', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явке', 8, 2), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявку', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явку', 8, 3), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявкой', tag=OpencorporaTag('NOUN,inan,femn sing,ablt'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явкой', 8, 4), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявкою', tag=OpencorporaTag('NOUN,inan,femn sing,ablt,V-oy'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явкою', 8, 5), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявке', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явке', 8, 6), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявки', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явки', 8, 7), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявок', tag=OpencorporaTag('NOUN,inan,femn plur,gent'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явок', 8, 8), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявкам', tag=OpencorporaTag('NOUN,inan,femn plur,datv'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явкам', 8, 9), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявки', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явки', 8, 10), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявками', tag=OpencorporaTag('NOUN,inan,femn plur,ablt'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явками', 8, 11), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявках', tag=OpencorporaTag('NOUN,inan,femn plur,loct'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явках', 8, 12), (<UnknownPrefixAnalyzer>, 'бут')))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butyavka.lexeme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теги и граммемы в pymorphy2 записываются латиницей (например, NOUN). Но часто удобнее использовать кириллические названия граммем (например, СУЩ вместо NOUN). Чтобы получить тег в виде строки, записанной кириллицей, используйте свойство OpencorporaTag.cyr_repr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),))\n",
      "VERB,perf,intr plur,past,indc\n",
      "ГЛ,сов,неперех мн,прош,изъяв\n"
     ]
    }
   ],
   "source": [
    "print(p)\n",
    "print(p.tag)\n",
    "print(p.tag.cyr_repr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем преобразовать предложение в список нормализованных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Лемматизация – это процесс преобразования слова в его базовую форму. Разница между стемминг (stemming) и лемматизацией заключается в том, что лемматизация учитывает контекст и преобразует слово в его значимую базовую форму, тогда как стемминг просто удаляет последние несколько символов, что часто приводит к неверному значению и орфографическим ошибкам.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "test_words = [word.lower() for word in re.findall(r'[а-яА-Я]+', test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_morph = [morph.parse(word)[0].normal_form for word in test_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лемматизация\n",
      "это\n",
      "процесс\n",
      "преобразования -> преобразование\n",
      "слова -> слово\n",
      "в\n",
      "его -> он\n",
      "базовую -> базовый\n",
      "форму -> форма\n",
      "разница\n",
      "между\n",
      "стемминг\n",
      "и\n",
      "лемматизацией -> лемматизация\n",
      "заключается -> заключаться\n",
      "в\n",
      "том -> тот\n",
      "что\n",
      "лемматизация\n",
      "учитывает -> учитывать\n",
      "контекст\n",
      "и\n",
      "преобразует -> преобразовать\n",
      "слово\n",
      "в\n",
      "его -> он\n",
      "значимую -> значимый\n",
      "базовую -> базовый\n",
      "форму -> форма\n",
      "тогда\n",
      "как\n",
      "стемминг\n",
      "просто\n",
      "удаляет -> удалять\n",
      "последние -> последний\n",
      "несколько\n",
      "символов -> символ\n",
      "что\n",
      "часто\n",
      "приводит -> приводить\n",
      "к\n",
      "неверному -> неверный\n",
      "значению -> значение\n",
      "и\n",
      "орфографическим -> орфографический\n",
      "ошибкам -> ошибка\n"
     ]
    }
   ],
   "source": [
    "for word, form in zip(test_words, test_morph):\n",
    "    if word != form:\n",
    "        print(word, '->', form)\n",
    "    else:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rogov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")  \n",
    "# Скачиваем нужные нам наборы. Необязательно скачивать все. Нам пока что нужен только\n",
    "# corpora stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем использовать nltk для получения списка стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # Импортируем список стол-слов\n",
    "print(stopwords.words(\"english\")) # Стоп-слова для английского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"russian\")) # Стоп-слова для русского языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are those news fake or real?\n",
    "У нас есть датасет состоящий из заголовка новости, текста новости и лейбла, который показывает фейковая это новость или реальная.\n",
    "\n",
    "Нашей задачей будет натренировать модель, чтобы она могла определить фейковая новость или реальная."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/rogovich/Data/master/data/fake_or_real_news.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Три основных столбца называются \"title\", \"text\", и \"label\".\n",
    "\n",
    "Разделим наши данные на обучающую и тестовую выборки, чтобы потом можно было проверить насколько качественно работает наш алгоритм. За y обозначим наш лейбл. За X саму колонку с текстом новостей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['label']\n",
    "X = data[['text']] # двойные скобки, чтобы она осталась датафреймом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для разделения данных на тестовую и обучающую выборки, воспользуемся функцией train_test_split из модуля sklearn. Как видно из названия она делит выборку на части и размещает данные по выбранным массивам. Test_size указывает на то, какую часть выборки нужно отложить для теста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы разбили данные и получили обучающую выборку, посмотрим как выглядит текст новости. Выведем любую новость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4244, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2091, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following news of FBI Director James Comey’s decision to reopen the investigation into Hillary Clinton’s use of a private email server during her tenure at the State Department, federal law enforcement officials have come forward with new details on recently discovered evidence in the case.\n",
      "More on this: DEVELOPING: FBI Reopens Investigation into Clinton Emails After New ‘Pertinent’ Evidence Discovered \n",
      "While Director Comey declined to provide specific details on what the newly discovered Clinton emails contained, federal law enforcements officials speaking under anonymity have explained that the emails were found on the personal devices of Clinton aide Huma Abedin and disgraced former Congressman Anthony Weiner.\n",
      "The discovery came as part of investigation into yet another Weiner ‘sexting’ scandal, this time after he was alleged to have been engaging in sexually explicit conversations with an underage female.\n",
      "More on the latest Weiner scandal: ‘Carlos Danger’ Strikes Again: New Reports Allege Anthony Weiner Knowingly Engaged in Sexually Explicit Conversations with 15 Year Old Female Online \n",
      "As reported by the New York Times, authorities discovered ‘pertinent’ emails related to the Clinton investigation on personal electronic devices belonging to Abedin and Weiner that had been seized by investigators as part of the investigation in Weiner’s alleged inappropriate conversations with a child.\n",
      "Via NYT \n",
      "Federal law enforcement officials said Friday that the new emails uncovered in the closed investigation into Hillary Clinton ’s use of a private email server were discovered after the F.B.I. seized electronic devices belonging to Huma Abedin, an aide to Mrs. Clinton, and her husband, Anthony Weiner.\n",
      "The F.B.I. told Congress that it had uncovered new emails related to the closed investigation into whether Mrs. Clinton or her aides had mishandled classified information, potentially reigniting an issue that has weighed on the presidential campaign and offering a lifeline to Donald J. Trump less than two weeks before the election.\n",
      "More via FoxNews \n",
      "Watch the latest video at video.foxnews.com \n",
      "We will continue to update as new details surface.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_train.iloc[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда внутри текста мы можем иногда увидить HTML тэги (например \"br\"), аббревиатуры, пунктуацию, которые являются распространенными проблемы при обработке текста из Интернета. Но вроде в наших данных этого нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Продолжим с новостями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее документацию CountVectorizer можно почитать тут:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем объект \"CountVectorizer\", метод для работы с bag of words\n",
    "# из scikit-learn\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             stop_words = 'english',   \\\n",
    "                             max_features = 1000) \n",
    "                                # max_features = ограничиваем максимальное\n",
    "                                # количество слов-признаков для ускорения\n",
    "                                # работы алгоритма. Если не ограничивать -\n",
    "                                # количество признаков будет равно всем\n",
    "                                # уникальным словам в нашем корпусе выборки\n",
    "\n",
    "# fit_transform() делает две вещи: Сначала он фитит модель\n",
    "# и изучает словарь; Потом трансформирует нашу обучающую выборку\n",
    "# в вектор признаков В fit_transform мы передаем список слов\n",
    "train_data_features = vectorizer.fit_transform(X_train['text'])\n",
    "\n",
    "# Используем удобные массивы из Numpy\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как выглядят наши обучающие данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4244, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ней 4244 строк и 1000 признаков (по одному на каждое слово в словаре).\n",
    "\n",
    "Теперь наша модель Bag of Words натренирована, посмотрим на получившийся словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '20', '2008', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '25', '26', '27', '28', '30', '40', '50', 'ability', 'able', 'abortion', 'access', 'according', 'account', 'accused', 'act', 'action', 'actions', 'actually', 'added', 'adding', 'address', 'administration', 'afghanistan', 'african', 'age', 'agency', 'agenda', 'agents', 'ago', 'agree']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно, можно вывести сколько раз слово встречается в словаре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578 agreement\n",
      "501 ahead\n",
      "667 air\n",
      "878 al\n",
      "473 allies\n",
      "488 allow\n",
      "437 allowed\n",
      "2466 america\n",
      "3488 american\n",
      "2037 americans\n",
      "666 announced\n",
      "494 answer\n",
      "1013 anti\n",
      "361 appear\n",
      "411 appeared\n",
      "435 appears\n",
      "409 approach\n",
      "379 april\n",
      "560 area\n",
      "401 areas\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Подсчитаем количество слов в наших данных\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# Выведем для каждого слова его количество\n",
    "for tag, count in zip(vocab[50:70], dist[50:70]):\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем построить логистическую модель\n",
    "В данный момент у нас есть численные значения признаков из обучающей выборки из Bag of Words и оригинальные лейблы новости, поэтому займемся обучением с учителем. Мы будем использовать классификатор логистическую регрессию. Этот алгоритм включен в scikit-learn. Я не буду вдавать в подробности работы алгоритма, здесь мне важно показать, что такая структура данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "reg = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "# Учим алгоритм предсказывает класс новости - fake или real\n",
    "reg.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем предсказание\n",
    "Осталось только запустить подготовленный алгоритм на нашей тестовой выборке и проверить результаты предсказания.\n",
    "\n",
    "Стоит отметить, что для тестовой выборки мы уже используем метод \"transform\", а не \"fit_transform\" в Bag of Words.\n",
    "\n",
    "Но для начала преобразуем нашу тестовую выборку в bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# очищаем тестовую подвыбоку\n",
    "\n",
    "# Конвертируем получившиеся bag of words в массив Numpy\n",
    "test_data_features = vectorizer.transform(X_test['text'])\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Делаем предсказание логистической регрессией\n",
    "result = reg.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим насколько точны были предсказания. Accuracy_score - находит долю правильно предсказанных классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8813964610234337"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "86% - Отличный результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "В больших текстах, некоторые слова могут встречаться очень часто (например “the”, “a”, “is” в аглийском) при этом неся мало информации об истинном содержании текста. Поэтому мы можем добавить веса словами.\n",
    "\n",
    "Tf-idf означает \"frequency-inverse document frequency\" (частотно-обратная частота документа) и представляет собой статистическую меру, используемую для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Вес некоторого слова пропорционален частоте употребления этого слова в документе и обратно пропорционален частоте употребления слова во всех документах коллекции. \n",
    "\n",
    "Большой вес в TF-IDF получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах.\n",
    "\n",
    "\n",
    "{\\displaystyle \\mathrm {tf} (t,d)={\\frac {n_{t}}{\\sum _{k}n_{k}}}} ,\n",
    "\n",
    "\n",
    "https://ru.wikipedia.org/wiki/TF-IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf слова \"заяц\" в первом документе 0.1\n",
      "tf слова \"заяц\" во втором документе 0.0\n",
      "tf слова \"заяц\" во третьем документе 0.18181818181818182\n",
      "idf для слова заяц 0.17609125905568124\n",
      "tf*idf слова \"заяц\" в первом документе 0.017609125905568124\n",
      "tf*idf слова \"заяц\" во втором документе 0.0\n",
      "tf*idf слова \"заяц\" во третьем документе 0.032016592555578406\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "example1 = 'в этом документе десять слов и одно из них заяц'.split()\n",
    "example2 = 'а в этом такого слова нет'.split()\n",
    "example3 = 'а здесь слова заяц снова есть есть заяц в этом документе'.split()\n",
    "\n",
    "print('tf слова \"заяц\" в первом документе', example1.count('заяц')/len(example1))\n",
    "print('tf слова \"заяц\" во втором документе', example2.count('заяц')/len(example2))\n",
    "print('tf слова \"заяц\" во третьем документе', example3.count('заяц')/len(example3))\n",
    "\n",
    "print('idf для слова заяц', math.log10(3/2))\n",
    "\n",
    "print('tf*idf слова \"заяц\" в первом документе', example1.count('заяц')/len(example1) * math.log10(3/2))\n",
    "print('tf*idf слова \"заяц\" во втором документе', example2.count('заяц')/len(example2) * math.log10(3/2))\n",
    "print('tf*idf слова \"заяц\" во третьем документе', example3.count('заяц')/len(example3) * math.log10(3/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем Tfidvectorizer для конвертации данных в веса tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Инициализируем объект TfidfVectorizer: tfidf_vectorizer\n",
    "# А также ограничили выдачу слов, которые мало встречаются в текстах, \n",
    "# использовав параметр max_df\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df = 0.7, min_df = 0.2) \n",
    "# Ограничиваем минимальный и максимальный порог встречаемости слова в текстах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренируем на тех же тестовых данных, что и CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тренируем классификатор\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train['text'])\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим что получили"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '2016', 'about', 'according', 'after']\n",
      "[[0.         0.         0.14443346 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.08832762 0.         0.        ]\n",
      " [0.         0.09497095 0.         ... 0.         0.07750256 0.22620801]\n",
      " [0.         0.         0.07640719 ... 0.         0.02931533 0.04278157]\n",
      " [0.         0.04410267 0.03126859 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Выведем 10 первых признаков\n",
    "print(tfidf_vectorizer.get_feature_names()[:5])\n",
    "\n",
    "# Выведем первые 5 векторов обучающей выборки\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vectorizer.get_feature_names()) # всего слов, удовлетворяющим нашим порогам было 212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, повлияет ли изменение метрики на качество алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.860832137733142"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Конвертируем получившиеся bag of words в массив Numpy\n",
    "\n",
    "reg.fit(tfidf_train, y_train)\n",
    "# Делаем предсказание логистической регрессией\n",
    "result = reg.predict(tfidf_test)\n",
    "accuracy_score(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного ухудшилось. Тут на самом деле играет роль то, что с данными в таком виде лучше работают другие алгоритмы, но это уже за пределами нашего курса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Для более наглядного примера, мы можем построить Confusion Matrix. Диагональные элементы матрицы указывают на количество верных попаданий для данного класса (Fake или Real в нашем случае). Остальные элементы в строке показывают сколько новостей ушли в другой класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[917, 154],\n",
       "       [137, 883]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "142 фейковых новостей были некорректно отмечены как правдивые. И 173 реальных новостей были отмечены как фейковые.\n",
    "\n",
    "Эта история важна, когда цена ошибки на одном из классов более высока. Например, наша алгоритм чаще называл фейковые новости реальными, чем наоборот. Если нам важно не назвать реальную новость фейковой, то мы ошиблись только 173 раз. А вот если наоборот, то цена ошибки выше."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
